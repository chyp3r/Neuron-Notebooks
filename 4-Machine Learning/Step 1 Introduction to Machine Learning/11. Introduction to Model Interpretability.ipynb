{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Model Interpretability\n",
    "\n",
    "Interpreting machine learning models is crucial for understanding how they make decisions. This section introduces the importance of model interpretability, common techniques, and code examples to make models more transparent.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Model Interpretability is Important](#1-why-model-interpretability-is-important)\n",
    "2. [Global vs. Local Interpretability](#2-global-vs-local-interpretability)\n",
    "3. [Techniques for Model Interpretability](#3-techniques-for-model-interpretability)\n",
    "    - 3.1 [Feature Importance](#31-feature-importance)\n",
    "    - 3.2 [Partial Dependence Plots (PDP)](#32-partial-dependence-plots-pdp)\n",
    "    - 3.3 [SHAP (SHapley Additive exPlanations)](#33-shap-shapley-additive-explanations)\n",
    "    - 3.4 [LIME (Local Interpretable Model-Agnostic Explanations)](#34-lime-local-interpretable-model-agnostic-explanations)\n",
    "\t\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Model Interpretability is Important\n",
    "\n",
    "Model interpretability ensures that machine learning models are transparent and their predictions are understandable. It’s especially critical in high-stakes domains such as healthcare, finance, and law, where decisions made by models must be justifiable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global vs. Local Interpretability\n",
    "\n",
    "- **Global Interpretability** refers to understanding the model's behavior as a whole. It explains how the model uses features to make predictions across all instances.\n",
    "- **Local Interpretability** focuses on understanding the model's decision for a specific instance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Techniques for Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feature Importance\n",
    "\n",
    "Feature importance assigns scores to input features based on how much they contribute to the model’s prediction. It helps to identify which features are most relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "importance = model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': data.feature_names,\n",
    "    'Importance': importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Partial Dependence Plots (PDP)\n",
    "Partial dependence plots show the relationship between a selected feature and the target variable while averaging out the effects of other features. It’s useful for visualizing how changes in a feature influence the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_partial_dependence(model, X, [0], feature_names=data.feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 SHAP (SHapley Additive exPlanations)\n",
    "SHAP values quantify how much each feature contributes to a specific prediction, providing both global and local interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values[1], X, feature_names=data.feature_names)\n",
    "\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][0], X[0], feature_names=data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 LIME (Local Interpretable Model-Agnostic Explanations)\n",
    "LIME explains the predictions of any model by approximating it locally with a simpler, interpretable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X, feature_names=data.feature_names, class_names=data.target_names, discretize_continuous=True)\n",
    "\n",
    "exp = explainer.explain_instance(X[0], model.predict_proba, num_features=2)\n",
    "\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
