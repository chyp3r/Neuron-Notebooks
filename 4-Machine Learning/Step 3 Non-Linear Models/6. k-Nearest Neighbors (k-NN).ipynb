{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (k-NN)\n",
    "\n",
    "The k-Nearest Neighbors (k-NN) algorithm is a simple, non-parametric, and lazy learning algorithm. It is used for both classification and regression tasks, but it is mostly used for classification problems.\n",
    "\n",
    "In this notebook, we will cover the intuition behind k-NN, its advantages, disadvantages, and demonstrate its implementation in Python using the `scikit-learn` library.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What is k-Nearest Neighbors?](#1-what-is-k-nearest-neighbors)\n",
    "2. [How k-NN Works](#2-how-k-nn-works)\n",
    "3. [Advantages and Disadvantages of k-NN](#3-advantages-and-disadvantages-of-k-nn)\n",
    "4. [Use Cases of k-NN](#4-use-cases-of-k-nn)\n",
    "5. [Implementing k-NN in Python](#5-implementing-k-nn-in-python)\n",
    "6. [Evaluating the k-NN Model](#6-evaluating-the-k-nn-model)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is k-Nearest Neighbors?\n",
    "\n",
    "The k-Nearest Neighbors (k-NN) algorithm is one of the simplest and most intuitive algorithms in machine learning. It works by finding the \"k\" closest data points (neighbors) to a given input and assigning the majority class label (for classification) or averaging the values (for regression).\n",
    "\n",
    "Key features:\n",
    "- **Lazy Learning**: k-NN does not learn an explicit model; instead, it memorizes the training data.\n",
    "- **Non-parametric**: It makes no assumptions about the underlying data distribution.\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How k-NN Works\n",
    "\n",
    "The basic steps of the k-NN algorithm are as follows:\n",
    "\n",
    "1. Choose the number of neighbors (k).\n",
    "2. Calculate the distance between the new data point and all other points in the dataset.\n",
    "3. Identify the k closest points (neighbors) to the new data point.\n",
    "4. For classification, assign the label that is most common among these neighbors.\n",
    "5. For regression, compute the average of the labels (values) of the k closest points.\n",
    "\n",
    "Distance metrics used:\n",
    "- **Euclidean Distance** (most common)\n",
    "- Manhattan Distance\n",
    "- Minkowski Distance\n",
    "\n",
    "Choosing the right value for **k** is critical. Small values for **k** can make the model sensitive to noise, while large values may lead to over-smoothing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantages and Disadvantages of k-NN\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **Simplicity**: k-NN is easy to understand and implement.\n",
    "- **No Training Phase**: As a lazy learner, it doesnâ€™t require training, which can be an advantage for real-time systems.\n",
    "- **Flexibility**: It can be used for both classification and regression problems.\n",
    "- **Adaptability**: It can handle multi-class classification problems.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "- **Computational Complexity**: As the dataset grows, k-NN becomes slow, especially for large datasets.\n",
    "- **Sensitive to Outliers**: k-NN can be influenced by noisy data points.\n",
    "- **Feature Scaling Required**: Since k-NN relies on distance metrics, feature scaling (standardization or normalization) is necessary.\n",
    "- **Choice of k**: Finding the optimal k can be challenging and may require cross-validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Use Cases of k-NN\n",
    "\n",
    "k-NN can be applied in the following scenarios:\n",
    "\n",
    "- **Image Recognition**: k-NN can be used to classify images based on similarities in pixel intensity values.\n",
    "- **Recommendation Systems**: It helps recommend items to users by finding similar users based on their preferences.\n",
    "- **Anomaly Detection**: k-NN can identify outliers in a dataset by analyzing the distance to the nearest neighbors.\n",
    "- **Healthcare**: It can be used to predict disease outcomes based on patient history and data from other patients with similar symptoms.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing k-NN in Python\n",
    "\n",
    "Below is a Python implementation of the k-NN algorithm using `scikit-learn`. We will use the famous **Iris** dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the k-NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-NN model: 1.00\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of k-NN model: {accuracy:.2f}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
