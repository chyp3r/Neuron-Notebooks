{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic machine learning algorithm based on Bayes' Theorem. It assumes that the presence (or absence) of a particular feature in a class is independent of the presence (or absence) of any other feature, hence the term \"naive.\" Despite its simplicity, it works well for various real-world problems.\n",
    "\n",
    "In this notebook, we will cover the intuition behind Naive Bayes, its types, advantages, disadvantages, and demonstrate its implementation in Python using the `scikit-learn` library.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What is Naive Bayes?](#1-what-is-naive-bayes)\n",
    "2. [Types of Naive Bayes Classifiers](#2-types-of-naive-bayes-classifiers)\n",
    "3. [Advantages and Disadvantages of Naive Bayes](#3-advantages-and-disadvantages-of-naive-bayes)\n",
    "4. [Use Cases of Naive Bayes](#4-use-cases-of-naive-bayes)\n",
    "5. [Implementing Naive Bayes in Python](#5-implementing-naive-bayes-in-python)\n",
    "6. [Evaluating the Naive Bayes Model](#6-evaluating-the-naive-bayes-model)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Naive Bayes?\n",
    "\n",
    "Naive Bayes is a simple and effective classification algorithm based on Bayes' Theorem with an assumption of independence among features. Bayes' Theorem calculates the probability of a class given a feature set by combining prior probability with the likelihood of the data:\n",
    "\n",
    "\\[\n",
    "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P(C|X) \\) is the posterior probability of class \\( C \\) given the feature \\( X \\).\n",
    "- \\( P(X|C) \\) is the likelihood of feature \\( X \\) given class \\( C \\).\n",
    "- \\( P(C) \\) is the prior probability of class \\( C \\).\n",
    "- \\( P(X) \\) is the prior probability of feature \\( X \\).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Naive Bayes Classifiers\n",
    "\n",
    "### 1. **Gaussian Naive Bayes**:\n",
    "   - Assumes that features follow a Gaussian (normal) distribution.\n",
    "   - Used for continuous data.\n",
    "\n",
    "### 2. **Multinomial Naive Bayes**:\n",
    "   - Used for discrete data, especially for text classification (e.g., document classification).\n",
    "   - Assumes the feature vectors represent frequencies or counts of different outcomes.\n",
    "\n",
    "### 3. **Bernoulli Naive Bayes**:\n",
    "   - Designed for binary/boolean features (e.g., 0 and 1).\n",
    "   - It is commonly used for document classification tasks where the input data is represented by binary features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantages and Disadvantages of Naive Bayes\n",
    "\n",
    "### Advantages:\n",
    "- **Simple and Fast**: Easy to implement and fast to compute.\n",
    "- **Works Well with Small Data**: Performs well on small datasets, especially for text classification.\n",
    "- **Handles Missing Data**: Can handle missing data points effectively by ignoring them.\n",
    "- **Performs Well with High Dimensional Data**: Especially in problems like spam detection, text classification, etc.\n",
    "\n",
    "### Disadvantages:\n",
    "- **Naive Assumption**: The assumption that features are independent is rarely true in real-world scenarios.\n",
    "- **Zero Probability Problem**: If a category is missing from the training set, the algorithm assigns it a zero probability, leading to potential issues.\n",
    "- **Sensitive to Imbalanced Data**: Performs poorly with imbalanced datasets unless properly managed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Cases of Naive Bayes\n",
    "\n",
    "Naive Bayes is commonly used in:\n",
    "- **Spam Detection**: Classifying emails as spam or not spam.\n",
    "- **Text Classification**: Sentiment analysis, document categorization.\n",
    "- **Medical Diagnosis**: Predicting diseases based on patient symptoms.\n",
    "- **Recommender Systems**: Personalizing user recommendations by classifying preferences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Naive Bayes in Python\n",
    "\n",
    "Below is a Python implementation of the Naive Bayes algorithm using `scikit-learn`. We will use the **Iris** dataset for this example, as itâ€™s a well-known dataset for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes model: 1.00\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Naive Bayes model: {accuracy:.2f}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
