{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machines (GBMs)\n",
    "\n",
    "Gradient Boosting Machines (GBMs) are a powerful ensemble learning technique for both classification and regression tasks. They build models sequentially, where each subsequent model attempts to correct the errors of the previous models. The models are trained using the gradient descent optimization method to minimize a specific loss function.\n",
    "\n",
    "In this notebook, we will cover the intuition behind GBMs, advantages, disadvantages, usage areas, and demonstrate an implementation in Python using `scikit-learn`'s `GradientBoostingClassifier`.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What is Gradient Boosting?](#1-what-is-gradient-boosting)\n",
    "2. [How Gradient Boosting Works](#2-how-gradient-boosting-works)\n",
    "3. [Advantages and Disadvantages of Gradient Boosting](#3-advantages-and-disadvantages-of-gradient-boosting)\n",
    "4. [Use Cases of Gradient Boosting Machines](#4-use-cases-of-gradient-boosting-machines)\n",
    "5. [Implementing Gradient Boosting in Python](#5-implementing-gradient-boosting-in-python)\n",
    "6. [Evaluating the Gradient Boosting Model](#6-evaluating-the-gradient-boosting-model)\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is an ensemble technique that creates a strong predictive model by combining the outputs of many weaker models, typically decision trees. The models are built sequentially, with each new model focusing on correcting the errors made by the previous models.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Boosting**: An iterative process that adjusts weights or residuals to improve prediction accuracy.\n",
    "- **Gradient Descent**: An optimization algorithm that minimizes the loss function of the model by iteratively adjusting the model parameters.\n",
    "\n",
    "The models are trained in sequence, and each model is fit to the residual errors of the previous model. This method minimizes a given loss function by combining the predictions of the individual models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. How Gradient Boosting Works\n",
    "\n",
    "1. **Initialization**: Start with an initial prediction for each training example. Typically, this is a mean value for regression or class probabilities for classification.\n",
    "2. **Build Weak Learner (Tree)**: Fit a weak learner, typically a shallow decision tree, to the residual errors or gradients.\n",
    "3. **Update Prediction**: Combine the weak learner’s predictions with the previous model’s predictions, typically using a weighted sum.\n",
    "4. **Iterate**: Repeat steps 2 and 3 until the model converges or the maximum number of weak learners (trees) is reached.\n",
    "\n",
    "### Loss Function and Gradient Descent\n",
    "In each iteration, the gradient of the loss function with respect to the model’s predictions is calculated. The new tree is fit to this gradient, effectively correcting the errors of the previous model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantages and Disadvantages of Gradient Boosting\n",
    "\n",
    "### Advantages:\n",
    "- **High Accuracy**: GBMs often achieve state-of-the-art performance for both classification and regression tasks.\n",
    "- **Handles Missing Data**: Gradient boosting can handle missing data well, particularly with tree-based models.\n",
    "- **Versatility**: It can be applied to both regression and classification tasks.\n",
    "- **Feature Importance**: Provides a measure of feature importance, which is helpful in understanding the model.\n",
    "\n",
    "### Disadvantages:\n",
    "- **Prone to Overfitting**: If not properly tuned, GBMs can overfit, especially when too many trees are added.\n",
    "- **Computationally Intensive**: Training GBMs can be slow and requires more memory than simpler models.\n",
    "- **Parameter Sensitivity**: GBMs have several hyperparameters (learning rate, number of trees, max depth, etc.) that must be carefully tuned for optimal performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Cases of Gradient Boosting Machines\n",
    "\n",
    "GBMs are used in a wide range of applications:\n",
    "- **Financial Fraud Detection**: To detect fraudulent transactions.\n",
    "- **Risk Modeling**: In insurance and finance to predict risks.\n",
    "- **Marketing Analytics**: To predict customer behavior and segmentation.\n",
    "- **Healthcare**: Used in predicting patient outcomes and personalized medicine.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Gradient Boosting in Python\n",
    "\n",
    "Let’s implement Gradient Boosting using the `GradientBoostingClassifier` from `scikit-learn` on the famous **Iris dataset**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbc.fit(X_train, y_train)\n",
    "y_pred = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gradient Boosting model: 1.00\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Gradient Boosting model: {accuracy:.2f}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
