{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling and Linear Models\n",
    "\n",
    "Feature scaling is a crucial preprocessing step in machine learning, especially for algorithms that are sensitive to the magnitudes of input features. In this notebook, we will explore how feature scaling impacts linear models such as Linear Regression and Logistic Regression.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Feature Scaling](#1-introduction-to-feature-scaling)\n",
    "2. [Types of Feature Scaling](#2-types-of-feature-scaling)\n",
    "    - Min-Max Scaling\n",
    "    - Standardization (Z-score normalization)\n",
    "3. [Feature Scaling in Linear Regression](#3-feature-scaling-in-linear-regression)\n",
    "4. [Feature Scaling in Logistic Regression](#4-feature-scaling-in-logistic-regression)\n",
    "5. [Feature Scaling Code Examples](#5-feature-scaling-code-examples)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Feature Scaling\n",
    "\n",
    "In machine learning, feature scaling is the process of bringing all the input features into the same scale. This helps prevent features with larger magnitudes from dominating the learning process and improves the convergence of optimization algorithms like Gradient Descent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Types of Feature Scaling\n",
    "\n",
    "### 2.1 Min-Max Scaling\n",
    "\n",
    "Min-Max scaling scales the data to a fixed range, typically between 0 and 1.\n",
    "\n",
    "\\[\n",
    "X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "\\]\n",
    "\n",
    "This method is sensitive to outliers.\n",
    "\n",
    "### 2.2 Standardization (Z-score normalization)\n",
    "\n",
    "Standardization rescales data to have a mean of 0 and a standard deviation of 1, which can be helpful when features have different units or distributions.\n",
    "\n",
    "\\[\n",
    "X_{scaled} = \\frac{X - \\mu}{\\sigma}\n",
    "\\]\n",
    "\n",
    "Where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation of the feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling in Linear Regression\n",
    "\n",
    "For Linear Regression, feature scaling is not strictly necessary, but it can improve convergence speed when using optimization techniques like Gradient Descent. Unscaled features can cause the optimizer to oscillate or converge slowly due to varying feature magnitudes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling in Logistic Regression\n",
    "\n",
    "Logistic Regression is also sensitive to feature scaling, particularly when regularization is used. Regularization methods like L1 or L2 penalize the model based on feature magnitudes, so scaling ensures that features contribute equally to the regularization term.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling Code Examples\n",
    "\n",
    "### 5.1 Min-Max Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaled Data:\n",
      " [[0.         0.        ]\n",
      " [0.2        0.16666667]\n",
      " [0.6        0.5       ]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1, 2], [2, 3], [4, 5], [6, 8]])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_array = scaler.fit_transform(array)\n",
    "print(\"Min-Max Scaled Data:\\n\", scaled_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized  Data:\n",
      " [[0.         0.        ]\n",
      " [0.2        0.16666667]\n",
      " [0.6        0.5       ]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_array2 = scaler.fit_transform(array)\n",
    "print(\"Standardized  Data:\\n\", scaled_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Feature Scaling in Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients: [0.95140628 0.668113  ]\n",
      "Test R-squared: 0.5065501493370641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.random.rand(100, 2)\n",
    "y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Model Coefficients:\", reg.coef_)\n",
    "print(\"Test R-squared:\", reg.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Feature Scaling in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", log_reg.score(X_test_scaled, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
